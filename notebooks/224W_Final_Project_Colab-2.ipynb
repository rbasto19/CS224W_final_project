{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOjZHspvGl7h"
      },
      "source": [
        "In this Colab, we will train an implementation of Dynaformer in PyG, mostly based on the Graphormer implementation at https://github.com/leffff/graphormer-pyg/blob/main/graphormer/layers.py, with some Dynaformer specific modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q16uTDsrqiNR"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiiafFCFG6_t"
      },
      "source": [
        "The files with the model and test dataset will be imported from a GitHub repository by the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85zVB6pvxAW2",
        "outputId": "fcad7e11-dc78-4212-94b4-bcedc34f72bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CS224W_final_project'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 32 (delta 15), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (32/32), 1.07 MiB | 1.24 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rbasto19/CS224W_final_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdkDSwugHEGH"
      },
      "source": [
        "Install necessary modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b49SwgROHGWn"
      },
      "source": [
        "Import necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "cO6bE5HKeBA-"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'layers'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[64], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Data, Batch\n\u001b[0;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlayers\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlayers\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmodel\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'layers'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch.utils.data import Subset\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "import networkx as nx\n",
        "from networkx import all_pairs_shortest_path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from torch_geometric.data import Data, Batch\n",
        "import layers as layers\n",
        "import model as model\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.nn.pool import global_mean_pool\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gRgOMXYHIor"
      },
      "source": [
        "Load dataset (we start with a small dataset for testing purposes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "7gpzHrd9eiYW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fetching sequence for 3uri: HTTPSConnectionPool(host='www.rcsb.org', port=443): Max retries exceeded with url: /fasta/entry/3uri (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x3208fc790>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n"
          ]
        }
      ],
      "source": [
        "with open('/Users/rbasto/Stanford projects/CS224W/refined-set-2020-5-5-5_train_val.pkl', 'rb') as f:\n",
        "  dataset = pickle.load(f)\n",
        "from pypdb import get_all_info\n",
        "from Bio import SeqIO\n",
        "import requests\n",
        "import time\n",
        "from io import StringIO\n",
        "\n",
        "def fetch_protein_sequence(pdb_id):\n",
        "    \"\"\"\n",
        "    Fetch the protein sequence from the PDB database.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Fetch FASTA sequence for the PDB ID\n",
        "        url = f\"https://www.rcsb.org/fasta/entry/{pdb_id}\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        # Parse the sequence from the response\n",
        "        sequences = [record.seq for record in SeqIO.parse(StringIO(response.text), \"fasta\")]\n",
        "        return str(sequences[0]) if sequences else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching sequence for {pdb_id}: {e}\")\n",
        "        return None\n",
        "    \n",
        "sequences = {}\n",
        "for i in range(len(dataset)):\n",
        "  dataset[i] = Data(**dataset[i].__dict__)  # allowing to use different pyg version\n",
        "  seq = fetch_protein_sequence(dataset[i].pdbid[:4])\n",
        "  if seq not in sequences.keys():\n",
        "    sequences[seq] = []\n",
        "  sequences[seq].append(dataset[i].pdbid[:4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10779baf0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/rbasto/miniconda3/envs/env224W/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
            "    def _clean_thread_parent_frames(\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ],
      "source": [
        "with open(\"/Users/rbasto/Downloads/md-refined2019-5-5-5/md-refined2019-5-5-5_test.pkl\", 'rb') as f:\n",
        "  dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_count = 0\n",
        "for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "        dataset[i][j] = Data(**dataset[i][j].__dict__)  # allowing to use different pyg version\n",
        "        dataset[i][j].x = dataset[i][j].x.to(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('/Users/rbasto/Stanford projects/CS224W/sequences_data-6-6-6.pkl', 'rb') as f:\n",
        "    sequences = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = {}\n",
        "for key in sequences.keys():\n",
        "    for i in range(len(sequences[key])):\n",
        "        sequences[key][i].x = sequences[key][i].x.to(torch.float32)\n",
        "\n",
        "with open('/Users/rbasto/Stanford projects/CS224W/sequences_data-6-6-6.pkl', 'wb') as f:\n",
        "    pickle.dump(sequences, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from copy import deepcopy\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from pypdb import get_all_info\n",
        "from Bio import SeqIO\n",
        "import requests\n",
        "import time\n",
        "from io import StringIO\n",
        "\n",
        "transform = T.Compose([T.remove_isolated_nodes.RemoveIsolatedNodes()])\n",
        "def remove_random_edges(graph, p):    \n",
        "    graph = deepcopy(graph)\n",
        "    num_edges = int(graph.edge_index.size()[1] / 2)\n",
        "    keep_edge = (torch.rand(num_edges) > p).reshape(-1,1)\n",
        "    keep_edge = torch.hstack((keep_edge, keep_edge)).flatten()\n",
        "    graph.edge_index = graph.edge_index.T[keep_edge].T\n",
        "    graph.edge_attr = graph.edge_attr[keep_edge]\n",
        "    graph = transform(graph)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6]\n",
            "[10000, 10000, 10000]\n"
          ]
        }
      ],
      "source": [
        "a = [1,2,3,4,5,6]\n",
        "b = random.sample(a, 3)\n",
        "for i in range(len(b)):\n",
        "    b[i] = 10000\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Data(x=[0, 9], edge_index=[2, 0], edge_attr=[0, 3], y=[1], pos=[0, 3], pdbid='1y0x_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0), Data(x=[0, 9], edge_index=[2, 0], edge_attr=[0, 3], y=[1], pos=[0, 3], pdbid='1xzx_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0)]\n",
            "[Data(x=[0, 9], edge_index=[2, 0], edge_attr=[0, 3], y=[1], pos=[0, 3], pdbid='1y0x_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0), Data(x=[0, 9], edge_index=[2, 0], edge_attr=[0, 3], y=[1], pos=[0, 3], pdbid='1xzx_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0)]\n"
          ]
        }
      ],
      "source": [
        "for val in sequences.values():\n",
        "    print(val)\n",
        "    for i in range(len(val)):\n",
        "        val[i] = remove_random_edges(val[i], 0.9)\n",
        "    print(val)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ids, val_ids = train_test_split([i for i in range(len(dataset))], test_size=0.3, random_state=42)\n",
        "train_loader = DataLoader(Subset(dataset, train_ids), batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(Subset(dataset, val_ids), batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'GlobalStorage' object has no attribute 'to_data_list'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39;49mto_data_list()[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto_data_list())\n\u001b[1;32m      3\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/env224W/lib/python3.10/site-packages/torch_geometric/data/data.py:561\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_store\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m    556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object was created by an older version of PyG. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this error occurred while loading an already existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset, remove the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directory in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroot folder and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, key)\n",
            "File \u001b[0;32m~/miniconda3/envs/env224W/lib/python3.10/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'to_data_list'"
          ]
        }
      ],
      "source": [
        "for data in train_loader:\n",
        "    print(data.to_data_list()[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[Data(x=[161, 9], edge_index=[2, 4880], edge_attr=[4880, 3], y=[1], pos=[161, 3], pdbid='4nw5_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0),\n",
              "  Data(x=[119, 9], edge_index=[2, 3172], edge_attr=[3172, 3], y=[1], pos=[119, 3], pdbid='5d9l_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0),\n",
              "  Data(x=[166, 9], edge_index=[2, 5032], edge_attr=[5032, 3], y=[1], pos=[166, 3], pdbid='4nw6_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0),\n",
              "  Data(x=[136, 9], edge_index=[2, 3708], edge_attr=[3708, 3], y=[1], pos=[136, 3], pdbid='4nus_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0)],\n",
              " [Data(x=[84, 9], edge_index=[2, 1920], edge_attr=[1920, 3], y=[1], pos=[84, 3], pdbid='4dmn_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0),\n",
              "  Data(x=[138, 9], edge_index=[2, 3806], edge_attr=[3806, 3], y=[1], pos=[138, 3], pdbid='4o55_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0),\n",
              "  Data(x=[148, 9], edge_index=[2, 4134], edge_attr=[4134, 3], y=[1], pos=[148, 3], pdbid='4o0j_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0),\n",
              "  Data(x=[136, 9], edge_index=[2, 3732], edge_attr=[3732, 3], y=[1], pos=[136, 3], pdbid='4gw6_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0),\n",
              "  Data(x=[145, 9], edge_index=[2, 4082], edge_attr=[4082, 3], y=[1], pos=[145, 3], pdbid='5kgw_pdbbind', num_node=[2], num_edge=[5], rfscore=[100], gbscore=[400], ecif=[1540], frame=-1, rmsd_lig=0.0, rmsd_pro=0.0)]]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.sample(list(Subset(dataset, train_ids)), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data in dataset:\n",
        "    print(data)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "tasks = []\n",
        "for val in sequences.values():\n",
        "    if len(val) > 3:\n",
        "        tasks.append(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpypdb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m found_pdbs \u001b[39m=\u001b[39m pypdb\u001b[39m.\u001b[39mQuery(\u001b[39m'\u001b[39m\u001b[39m6cvv\u001b[39m\u001b[39m'\u001b[39m, query_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstructure\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msearch()\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(found_pdbs[:\u001b[39m10\u001b[39;49m])\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "import pypdb\n",
        "found_pdbs = pypdb.Query('6cvv', query_type=\"structure\").search()\n",
        "print(found_pdbs[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT-iwgZbFjhE"
      },
      "source": [
        "Ligand-binding site visualization (visualization code generated with ChatGPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "vFcOMCzDAQNl",
        "outputId": "f0faff92-a0a0-4a97-82ff-e4e416c4df42"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 31\u001b[0m\n\u001b[1;32m     25\u001b[0m fig \u001b[39m=\u001b[39m go\u001b[39m.\u001b[39mFigure(data\u001b[39m=\u001b[39m[node_trace] \u001b[39m+\u001b[39m edge_traces)\n\u001b[1;32m     26\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(scene\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(\n\u001b[1;32m     27\u001b[0m     xaxis\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     28\u001b[0m     yaxis\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mY\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     29\u001b[0m     zaxis\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mZ\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     30\u001b[0m ))\n\u001b[0;32m---> 31\u001b[0m fig\u001b[39m.\u001b[39;49mshow()\n",
            "File \u001b[0;32m~/miniconda3/envs/env224W/lib/python3.10/site-packages/plotly/basedatatypes.py:3410\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3377\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3406\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3407\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3408\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[0;32m-> 3410\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/env224W/lib/python3.10/site-packages/plotly/io/_renderers.py:394\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    398\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
          ]
        }
      ],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "data = dataset[0]\n",
        "node_trace = go.Scatter3d(\n",
        "    x=data.pos[:, 0],\n",
        "    y=data.pos[:, 1],\n",
        "    z=data.pos[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(size=5, color='blue'),\n",
        "    text=[f'Node {i}' for i in range(data.pos.size(0))]\n",
        ")\n",
        "\n",
        "# Lines for edges\n",
        "edge_traces = []\n",
        "for i, j in data.edge_index.t():\n",
        "    edge_traces.append(go.Scatter3d(\n",
        "        x=[data.pos[i, 0], data.pos[j, 0]],\n",
        "        y=[data.pos[i, 1], data.pos[j, 1]],\n",
        "        z=[data.pos[i, 2], data.pos[j, 2]],\n",
        "        mode='lines',\n",
        "        line=dict(color='black', width=2),\n",
        "    ))\n",
        "\n",
        "# Combine the traces and plot\n",
        "fig = go.Figure(data=[node_trace] + edge_traces)\n",
        "fig.update_layout(scene=dict(\n",
        "    xaxis=dict(title='X'),\n",
        "    yaxis=dict(title='Y'),\n",
        "    zaxis=dict(title='Z'),\n",
        "))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG9XooV-HVRl"
      },
      "source": [
        "Create the model and split dataset. Here we are using the default parameters in the Graphormer implementation for testing purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2C3pwNp5lTw3"
      },
      "outputs": [],
      "source": [
        "graph_model = model.Graphormer(\n",
        "    num_layers=2,\n",
        "    input_node_dim=dataset[0].num_node_features,\n",
        "    node_dim=32,\n",
        "    input_edge_dim=dataset[0].num_edge_features,\n",
        "    edge_dim=8,\n",
        "    output_dim=1,\n",
        "    n_heads=2,\n",
        "    ff_dim=32,\n",
        "    max_in_degree=4,\n",
        "    max_out_degree=4,\n",
        "    max_path_distance=4,\n",
        "    num_heads_spatial=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6MvXSiV5ec20"
      },
      "outputs": [],
      "source": [
        "test_ids, train_ids = train_test_split([i for i in range(len(dataset))], test_size=0.5, random_state=42)\n",
        "train_loader = DataLoader(Subset(dataset, train_ids), batch_size=4)\n",
        "test_loader = DataLoader(Subset(dataset, test_ids), batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRv-SfjlHemA"
      },
      "source": [
        "Define optimizer, loss function, and start training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xud-eWJYq65H"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(graph_model.parameters(), lr=1e-4)\n",
        "loss_function = torch.nn.L1Loss(reduction=\"sum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "B5VJb8rQrPA4",
        "outputId": "af4e3eaf-c5a8-4ff0-c52a-a0d8259b3739"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 3/156 [01:49<1:32:54, 36.43s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m loss_function(output, y)\n\u001b[1;32m      9\u001b[0m batch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 10\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     11\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(graph_model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/miniconda3/envs/env224W/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    583\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/env224W/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    355\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/env224W/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    graph_model.train()\n",
        "    batch_loss = 0.0\n",
        "    for batch in tqdm(train_loader):\n",
        "        y = batch.y\n",
        "        optimizer.zero_grad()\n",
        "        output = global_mean_pool(graph_model(batch), batch.batch)\n",
        "        loss = loss_function(output, y)\n",
        "        batch_loss += loss.item()\n",
        "        print(batch_loss)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(graph_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "    print(\"TRAIN_LOSS\", batch_loss / len(train_ids))\n",
        "\n",
        "    model.eval()\n",
        "    batch_loss = 0.0\n",
        "    for batch in tqdm(test_loader):\n",
        "        y = batch.y\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = global_mean_pool(graph_model(batch), batch.batch)\n",
        "            loss = loss_function(output, y)\n",
        "\n",
        "        batch_loss += loss.item()\n",
        "\n",
        "    print(\"EVAL LOSS\", batch_loss / len(test_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "coords = torch.tensor([[-3.8230,  3.0390,  6.5360],\n",
        "        [-3.4140,  3.3890,  5.3210],\n",
        "        [-5.6860,  1.5970,  5.7010],\n",
        "        [-4.6260,  1.7350,  6.5600],\n",
        "        [-4.3070,  0.6970,  7.4450]])\n",
        "norms = torch.tensor([[66.5701],\n",
        "        [51.4538],\n",
        "        [67.3824],\n",
        "        [67.4437],\n",
        "        [74.4641]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataBatch(x=[508, 9], edge_index=[2, 9260], edge_attr=[9260, 3], y=[4], pos=[508, 3], pdbid=[4], num_node=[8], num_edge=[20], rfscore=[400], gbscore=[1600], ecif=[6160], frame=[4], rmsd_lig=[4], rmsd_pro=[4], batch=[508], ptr=[5])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Graphormer(\n",
              "  (node_in_lin): Linear(in_features=9, out_features=32, bias=True)\n",
              "  (edge_in_lin): Linear(in_features=3, out_features=8, bias=True)\n",
              "  (centrality_encoding): CentralityEncoding()\n",
              "  (spatial_encoding): SpatialEncoding()\n",
              "  (layers): ModuleList(\n",
              "    (0-1): 2 x GraphormerEncoderLayer(\n",
              "      (attention): GraphormerMultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-1): 2 x GraphormerAttentionHead(\n",
              "            (edge_encoding): EdgeEncoding()\n",
              "            (q): Linear(in_features=32, out_features=32, bias=True)\n",
              "            (k): Linear(in_features=32, out_features=32, bias=True)\n",
              "            (v): Linear(in_features=32, out_features=32, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (linear): Linear(in_features=64, out_features=32, bias=True)\n",
              "      )\n",
              "      (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=32, out_features=32, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (node_out_lin): Linear(in_features=32, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   nan, 1.3289, 2.4995, 1.5316, 2.5584],\n",
              "        [1.3289, 0.0092, 2.9185, 2.3958, 3.5434],\n",
              "        [2.4995, 2.9185, 0.0000, 1.3713, 2.3986],\n",
              "        [1.5316, 2.3958, 1.3713, 0.0000, 1.4009],\n",
              "        [2.5584, 3.5434, 2.3986, 1.4009, 0.0055]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norms - 2 * coords @ coords.T + norms.T"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
